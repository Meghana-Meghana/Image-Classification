{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Scenario:\n",
    "In this project we have a dataset containing different images of cats and dogs. There are 5000 images of both cats and dogs. We need to use these 10000 images to train the neural network model to predict whether the image is of a cat or a dog. \n",
    "\n",
    "To achieve this goal, we will train a **Convolutional Neural Network (CNN)**. We will build a CNN using one of the deep learning libraries, **Keras**.\n",
    "\n",
    "We first divide the available dataset into training and validation set. We use 80% (4000 images of cats + 4000 images of dogs = 8000 images) of the dataset as training set and 20% (1000 images of cats + 1000 images of dogs = 2000 images) as validation set.\n",
    "\n",
    "## Import Libraries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the layers in CNN are arranged sequentially. Hence, we import the Sequential model from the Keras package. CNN is composed of following layers:\n",
    "* **Convolutional layer:** \n",
    "Conv2D helps in adding this layer to our CNN model.Since we are dealing with images in our project, we use Conv2D.\n",
    "* **Pooling layer:** \n",
    "Pooling can be of different types like Max, Sum, Average, etc. In our project we are using MaxPooling.\n",
    "* **Flatten layer:** \n",
    "This layer converts a 2-D matrix a 1-D array that acts as input to the next layer.\n",
    "* **Fully Connected layer:** \n",
    "Dense helps in adding this layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_datapath = os.getcwd() + '/dataset/training_set'\n",
    "validation_datapath = os.getcwd() + '/dataset/validation_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# PART 1 - Building a CNN model #\n",
    "# ----------------------------- #\n",
    "\n",
    "# Initialising the CNN \n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding Convolution layer \n",
    "classifier.add(Conv2D(32,(3,3),input_shape = (64, 64, 3),activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Adding second Convolution layer \n",
    "classifier.add(Conv2D(32,(3,3),activation = 'relu' ))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Adding Flatten layer\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding fully connected layer\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN model\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Convolutional layer of our CNN model consists of 32 filters/ kernels/ feature detectors of size/ shape (3 X 3). \n",
    "The dataset contains images of different sizes and formats. Hence, we need to first convert all the images to a single common format. This is done by the input_shape parameter. Now, all our images from the dataset will have the same format of 64X64. 3 in the input_shape parameter indicates that we are dealing with the color images. 1 is used incase of black and white images (greyscale images).\n",
    "\n",
    "The second Convolutional layer has been added for the sake of improving accuracy of the training. Here, we do not need the input_shape parameter because, the inputs are already forced to be of uniform format in the first layer. This means that after the first layer all the outcomes henceforth will have the uniform format.\n",
    "\n",
    "The Convolutional layer is accompanied with the Pooling layer to downsample the size of the feature maps resulting after convolution operation. In our CNN model, we are using the pooling window of size (2X2).\n",
    "\n",
    "Like the name suggests, the Flatten layer flattens i.e., it converts the 2D matrices resulting from the previous layer into a 1D array, which acts as an input layer of the following Fully Connected Neural Network. In our CNN model, we are using a single hidden layer with 128 neurons and relu as activation function of the hidden layer. Since, there are only two categories (cats and dogs) in our classification problem, one neuron in the output layer would be sufficient and the sigmoid activation function suits the best (in case of multi-class classification softmax function will be a better alternative). \n",
    "\n",
    "The loss function is binary crossentropy due to the binary classification of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------- #\n",
    "# PART 2 - Data(Image) Augmentation  #\n",
    "# ---------------------------------- #\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        training_datapath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        validation_datapath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "training_set.class_indices   #gives the indices of the two classes of outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is done when we have small sample of training data. In our project we only have 10000 images for training the model. Small sample of training data often leads to overfitting of the model. Hence, we perform image augmentation before fitting the model to our training data.Image augmentation performs random transformations on the available data images in every batch like rotation, shearing, zooming and resizing and hence, increasing our training data. Thus, avoids overfitting of data.\n",
    "\n",
    "Here, the target_size of the augmented images is (64X64) same as that of our input images and the class mode is binary because of the binary classification of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 1630s 204ms/step - loss: 0.3441 - acc: 0.8414 - val_loss: 0.5425 - val_acc: 0.8115\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 1502s 188ms/step - loss: 0.0979 - acc: 0.9632 - val_loss: 0.8867 - val_acc: 0.8095\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 1429s 179ms/step - loss: 0.0473 - acc: 0.9834 - val_loss: 1.2123 - val_acc: 0.7900\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 1388s 173ms/step - loss: 0.0336 - acc: 0.9884 - val_loss: 1.2660 - val_acc: 0.7935\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 1391s 174ms/step - loss: 0.0282 - acc: 0.9903 - val_loss: 1.1956 - val_acc: 0.8140\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 1398s 175ms/step - loss: 0.0229 - acc: 0.9922 - val_loss: 1.3392 - val_acc: 0.8035\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 1403s 175ms/step - loss: 0.0202 - acc: 0.9933 - val_loss: 1.5073 - val_acc: 0.7840\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 1405s 176ms/step - loss: 0.0180 - acc: 0.9941 - val_loss: 1.4777 - val_acc: 0.8060\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 1402s 175ms/step - loss: 0.0167 - acc: 0.9947 - val_loss: 1.5065 - val_acc: 0.7980\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 1415s 177ms/step - loss: 0.0149 - acc: 0.9954 - val_loss: 1.4345 - val_acc: 0.8010\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 1387s 173ms/step - loss: 0.0139 - acc: 0.9957 - val_loss: 1.5815 - val_acc: 0.7960\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 1361s 170ms/step - loss: 0.0125 - acc: 0.9960 - val_loss: 1.3824 - val_acc: 0.8025\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 1472s 184ms/step - loss: 0.0127 - acc: 0.9960 - val_loss: 1.6087 - val_acc: 0.7920\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 1736s 217ms/step - loss: 0.0107 - acc: 0.9967 - val_loss: 1.4775 - val_acc: 0.8120\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 1712s 214ms/step - loss: 0.0108 - acc: 0.9965 - val_loss: 1.7112 - val_acc: 0.8010\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 1575s 197ms/step - loss: 0.0094 - acc: 0.9970 - val_loss: 1.6227 - val_acc: 0.8005\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 1410s 176ms/step - loss: 0.0089 - acc: 0.9972 - val_loss: 1.6352 - val_acc: 0.8035\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 1490s 186ms/step - loss: 0.0089 - acc: 0.9973 - val_loss: 1.8427 - val_acc: 0.7960\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 1600s 200ms/step - loss: 0.0096 - acc: 0.9971 - val_loss: 1.7197 - val_acc: 0.8095\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 1680s 210ms/step - loss: 0.0083 - acc: 0.9975 - val_loss: 1.7051 - val_acc: 0.8075\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 1613s 202ms/step - loss: 0.0078 - acc: 0.9977 - val_loss: 1.8319 - val_acc: 0.8005\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 1413s 177ms/step - loss: 0.0077 - acc: 0.9977 - val_loss: 1.7507 - val_acc: 0.8005\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 1392s 174ms/step - loss: 0.0081 - acc: 0.9976 - val_loss: 1.6434 - val_acc: 0.8165\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 1405s 176ms/step - loss: 0.0068 - acc: 0.9980 - val_loss: 1.6921 - val_acc: 0.8150\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 1528s 191ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 1.7595 - val_acc: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d11dbd9e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------- #\n",
    "# PART 3 - Fitting/ Training the CNN model #\n",
    "# ---------------------------------------- #\n",
    "\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000,\n",
    "        epochs=25,\n",
    "        validation_data=validation_set,\n",
    "        validation_steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model can be further increased by..\n",
    "*  adding more convolutional layers\n",
    "*  adding more hidden layers in the fully connected neural network.\n",
    "*  tweaking the number of neurons in the hidden layers.\n",
    "*  increasing the input size. Because, higher the input size higher will be the pixel information and better will be the results.\n",
    "\n",
    "We need to remember that this could also be computationally expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction = dog\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------  #\n",
    "# PART 4 - Making New Prediction   #\n",
    "# -------------------------------- #\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image_1_path = os.getcwd() + '/dataset/test_set/cat_or_dog_1.jpg'\n",
    "test_image_2_path = os.getcwd() + '/dataset/test_set/cat_or_dog_2.jpg'\n",
    "\n",
    "test_image_1 = image.load_img(test_image_1_path, target_size = (64,64))\n",
    "test_image_1 = image.img_to_array(test_image_1)\n",
    "test_image_1 = np.expand_dims(test_image_1, axis = 0)\n",
    "\n",
    "result = classifier.predict(test_image_1)\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    print(\"prediction = dog\")\n",
    "else:\n",
    "    print(\"prediction = cat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
